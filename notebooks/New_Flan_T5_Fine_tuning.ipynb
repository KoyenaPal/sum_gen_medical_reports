{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bbwl6E1E205R",
    "outputId": "9737acfd-29c0-4908-cb54-4391aad7875e"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install rich[jupyter]\n",
    "!pip install pandas\n",
    "!pip install numpy==1.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71II2dYQyoMO",
    "outputId": "39add3b8-78d8-45d7-90e6-52c85000f517"
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6nEben93JAk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_file = data_dir + \"setup8_training.csv\"\n",
    "num_examples=7000\n",
    "df = pd.read_csv(data_file).dropna()[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Suxgy7wC4IqL",
    "outputId": "26633fde-9b0e-4ffe-b94d-d342871eac54"
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYfBicZQ59Jf"
   },
   "outputs": [],
   "source": [
    "prefix = \"Summarize the following text as a discharge summary report: \"\n",
    "df[\"source\"] = prefix +df[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "81f4PKa1F6aM",
    "outputId": "fd820497-ecde-4ead-b729-b6a656bd2bbb"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wB441x104K-o"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "console=Console(record=True)\n",
    "\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "    console=Console()\n",
    "    table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "    console.print(table)\n",
    "    \n",
    "    \n",
    "    \n",
    "training_logger = Table(Column(\"Epoch\", justify=\"center\" ),\n",
    "                        Column(\"Steps\", justify=\"center\"),\n",
    "                        Column(\"Loss\", justify=\"center\"),\n",
    "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlYaKW9h4ai_"
   },
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vLQPGAn4v17"
   },
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and \n",
    "    loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XD2qL87Wsn19"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEFAULT_DEVICE = \"mps\"\n",
    "def create_csv(all_sentences, targets, file_to_write):\n",
    "    sources = []\n",
    "    fieldnames = [\"source\", \"target\"]\n",
    "    test_array = []\n",
    "    with open(file_to_write, 'w') as csvfile:\n",
    "        csvwriter = csv.DictWriter(csvfile, delimiter=',', fieldnames=fieldnames)\n",
    "        for t in range(len(targets)):\n",
    "            test_array.append({\"source\": all_sentences[t], \"target\": targets[t]})\n",
    "        csvwriter.writerow(dict((fn,fn) for fn in fieldnames))\n",
    "        for row in test_array:\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def generate_summaries(lns, metric, model, tokenizer, result_file_name, batch_size=1, device=DEFAULT_DEVICE):\n",
    "    prefix = \"Summarize the following text as a discharge summary report: \"\n",
    "    article_batches = list(chunks(lns['source'].values, batch_size))\n",
    "    article_batches = list(map(lambda x: list(prefix + x), article_batches))\n",
    "    target_batches = list(chunks(lns['target'].values, batch_size))\n",
    "    ls_prediction = []\n",
    "    ls_groundtruth = []\n",
    "\n",
    "    dec_batches_untokenized = []\n",
    "    target_batches_untokenized = []\n",
    "\n",
    "    for article_batch, target_batch in tqdm(zip(article_batches, target_batches)\n",
    "    , total=len(article_batches)):\n",
    "        dct = tokenizer(article_batch,\n",
    "                        max_length=512,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        return_tensors=\"pt\")\n",
    "        dec = []\n",
    "        summaries = model.generate(input_ids=dct[\"input_ids\"].to(device),\n",
    "                                     attention_mask=dct[\"attention_mask\"].to(device),\n",
    "                                     num_beams=10,min_length=120,max_length=512,repetition_penalty=2.5, length_penalty=1.0)\n",
    "        dec = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]  \n",
    "        dec_batches_untokenized.append(dec)\n",
    "        target_batches_untokenized.append(target_batch)\n",
    "        ls_prediction.extend(dec)\n",
    "        ls_groundtruth.extend(target_batch)\n",
    "        \n",
    "    ls_prediction_tokenized = coreNLP_tokenizer(ls_prediction)\n",
    "    target_batch_tokenized = coreNLP_tokenizer(ls_groundtruth)\n",
    "\n",
    "    metric.add_batch(predictions=ls_prediction_tokenized, references=target_batch_tokenized)\n",
    "\n",
    "    score = metric.compute()\n",
    "    str_now = str(datetime.now())\n",
    "    \n",
    "    create_csv(ls_groundtruth, ls_prediction, result_file_name)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ay6xvF7n2hIW",
    "outputId": "e0ff67bb-c278-47d7-cba2-3b10aab95e60",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets\n",
    "\n",
    "from datasets import list_datasets, list_metrics, load_dataset, load_metric\n",
    "metrics_list = list_metrics()\n",
    "len(metrics_list)\n",
    "print (metrics_list)\n",
    "rouge_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nkj6wIMt40RK"
   },
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _%10==0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUBykK-A43DF"
   },
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "            generated_ids = model.generate(\n",
    "                            input_ids = ids,\n",
    "                            attention_mask = mask, \n",
    "                            max_length=512, \n",
    "                            num_beams=10,\n",
    "                            repetition_penalty=2.5, \n",
    "                            length_penalty=2.0, \n",
    "                            early_stopping=True\n",
    "                            )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%10==0:\n",
    "                console.print(f'Completed {_}')\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483,
     "referenced_widgets": [
      "650e91d1d777467fa69f28d620ea6cc7",
      "905342684e1e47538f98ce293a567493",
      "6b1a8cc064e7487faa3c918b11663155",
      "d5036e304a8d439084a0a1baf73bb788",
      "94262c51a36a417d85a4673f7ed6a13b",
      "8b0e8317057d4891b0c7c117d1d4a65a",
      "14d5e1ad2cb444158d8a730bf4b096b8",
      "3784ec6ee295465fa4746590f1439ff7",
      "d66989bd79274828bc829fbf7cb18c2b",
      "e610792c40ee40a1814926ac16449db3",
      "049270d9315948d9b91a3b06eae0c9f6"
     ]
    },
    "id": "9-vTU_qVm6Rd",
    "outputId": "0eb7caf5-dd6c-4857-ac99-a2777d8c5a02",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n",
    "!pip install stanza\n",
    "\n",
    "import stanza\n",
    "\n",
    "# Download the Stanford CoreNLP package with Stanza's installation command\n",
    "# This'll take several minutes, depending on the network speed\n",
    "corenlp_dir = './corenlp'\n",
    "stanza.install_corenlp(dir=corenlp_dir)\n",
    "\n",
    "# Set the CORENLP_HOME environment variable to point to the installation location\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = corenlp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XVbkpQy2kha"
   },
   "outputs": [],
   "source": [
    "# Import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "\n",
    "def coreNLP_tokenizer(inputDocsList):\n",
    "    tokenizedDocsList = []\n",
    "    with CoreNLPClient(annotators=\"tokenize ssplit pos lemma ner depparse\".split(), memory='4G', endpoint='http://localhost:9001', be_quiet=True) as client:\n",
    "        for d in inputDocsList:\n",
    "            ann = client.annotate(d)\n",
    "            sentence = ann.sentence[0]\n",
    "            tokenizedDocsList.append(' '.join([token.word.lower() for token in sentence.token]))\n",
    "        return tokenizedDocsList\n",
    "\n",
    "\n",
    "def test(model, tokenizer, test_file_path, result_file_name, production=False):\n",
    "    df = pd.read_csv(test_file_path)\n",
    "    df.dropna()\n",
    "    df['source'] = df['source'].astype(str)\n",
    "    df['target'] = df['target'].astype(str)\n",
    "    df = df.iloc[:1000,:]\n",
    "    score = generate_summaries(df, rouge_metric, model, tokenizer, result_file_name=result_file_name)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tw4RW_qO4_8T"
   },
   "outputs": [],
   "source": [
    "def T5Trainer(dataframe, source_text, target_text, test_file_path, result_file_name, model_params, output_dir=\"./models/\" ):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text,target_text]]\n",
    "    display_df(dataframe.head(2))\n",
    "\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "    val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"VAL Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "    val_params = {\n",
    "        'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f'[Initiating Fine Tuning]...\\n')\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "      \n",
    "    console.log(f\"[Saving Model]...\\n\")\n",
    "    #Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "    # evaluating test dataset\n",
    "    console.log(f\"[Initiating Validation]...\\n\")\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
    "  \n",
    "    console.save_text(os.path.join(output_dir,'logs.txt'))\n",
    "  \n",
    "    console.log(f\"[Validation Completed.]\\n\")\n",
    "    console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
    "    console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n",
    "\n",
    "    console.log(f\"[Now onto testing]\\n\")\n",
    "    score = test(model, tokenizer, test_file_path, result_file_name)\n",
    "    return model, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxCpQwD8PDIs"
   },
   "outputs": [],
   "source": [
    "model_params={\n",
    "    \"MODEL\":\"google/flan-t5-base\", # model_type\n",
    "    \"TRAIN_BATCH_SIZE\":3,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":3,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":2e-5,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":120,   # max length of target text\n",
    "    \"SEED\": 200                     # set seed for reproducibility \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qijZoYeI55fM",
    "outputId": "ff045b42-7d85-431f-ea2c-6a6395b861ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"../outputs/\"\n",
    "test_file_path = data_dir + \"setup8_testing.csv\"\n",
    "result_file_name = output_dir + \"flan_t5_fine_tuned_setup8_may_8_testing.csv\"\n",
    "#model, score = T5Trainer(dataframe=df, source_text=\"source\", target_text=\"target\", test_file_path= test_file_path,result_file_name=result_file_name, model_params=model_params, output_dir=output_dir)\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir + \"model_files\")\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(output_dir + \"model_files\")\n",
    "model = model.to(device)\n",
    "score = test(model, tokenizer, test_file_path, result_file_name)\n",
    "print(score)\n",
    "# with open('../rouge_score_setup8_flan_t5_fine_tune.txt', 'w') as f:\n",
    "#     score = test(model, tokenizer, test_file_path, result_file_name)\n",
    "#     f.write(str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "049270d9315948d9b91a3b06eae0c9f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14d5e1ad2cb444158d8a730bf4b096b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3784ec6ee295465fa4746590f1439ff7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "650e91d1d777467fa69f28d620ea6cc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_905342684e1e47538f98ce293a567493",
       "IPY_MODEL_6b1a8cc064e7487faa3c918b11663155",
       "IPY_MODEL_d5036e304a8d439084a0a1baf73bb788"
      ],
      "layout": "IPY_MODEL_94262c51a36a417d85a4673f7ed6a13b"
     }
    },
    "6b1a8cc064e7487faa3c918b11663155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3784ec6ee295465fa4746590f1439ff7",
      "max": 506470124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d66989bd79274828bc829fbf7cb18c2b",
      "value": 506470124
     }
    },
    "8b0e8317057d4891b0c7c117d1d4a65a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "905342684e1e47538f98ce293a567493": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b0e8317057d4891b0c7c117d1d4a65a",
      "placeholder": "​",
      "style": "IPY_MODEL_14d5e1ad2cb444158d8a730bf4b096b8",
      "value": "Downloading https://huggingface.co/stanfordnlp/CoreNLP/resolve/main/stanford-corenlp-latest.zip: 100%"
     }
    },
    "94262c51a36a417d85a4673f7ed6a13b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5036e304a8d439084a0a1baf73bb788": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e610792c40ee40a1814926ac16449db3",
      "placeholder": "​",
      "style": "IPY_MODEL_049270d9315948d9b91a3b06eae0c9f6",
      "value": " 506M/506M [00:28&lt;00:00, 18.8MB/s]"
     }
    },
    "d66989bd79274828bc829fbf7cb18c2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e610792c40ee40a1814926ac16449db3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
