{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1650855029522,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"BNP9Sx3QUm8A","outputId":"bc4075a3-5778-4360-e3a0-feab9bb51c89"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1650855029523,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"88iulhT4UoOi","outputId":"03913741-2aa5-4d63-9897-8e04c3deeb95"},"outputs":[],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7608,"status":"ok","timestamp":1650855037128,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"B7EVTfMVdv4Z"},"outputs":[],"source":["# This run uses Pytorch Lightening to finetune the model\n","!pip install -q pytorch-lightning\n","!pip install -q transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15215,"status":"ok","timestamp":1650855052340,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"3fDnjiDoeZbK","outputId":"8a61c2bb-68ae-4a3a-bc80-3b8b04687866"},"outputs":[],"source":["# imports\n","import transformers\n","from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n","import pandas as pd\n","import numpy as np\n","\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","import torch\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","import math\n","import random\n","import re\n","\n","# install datasets\n","!pip install datasets\n","\n","from datasets import list_datasets, list_metrics, load_dataset, load_metric\n","\n","from pprint import pprint\n","\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","from tqdm import tqdm\n","\n","import argparse"]},{"cell_type":"markdown","metadata":{"id":"givMDERUB18s"},"source":["# Firing up Google Drive\n","Load up your google drive for loading the dataset for training and for saving model weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3297,"status":"ok","timestamp":1650855055624,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"Goj88fbbQ3Wo","outputId":"fc702ff9-a803-44b2-edf6-d1abcad78f96"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=False)\n","root_dir = \"/content/gdrive/My Drive/masters_thesis/\"\n","base_dir = root_dir"]},{"cell_type":"markdown","metadata":{"id":"-YqJ68pEB-_g"},"source":["# Pytorch Lightning for running the training"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1650855055625,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"cPxWynRmeGlL"},"outputs":[],"source":["class LitModel(pl.LightningModule):\n","  # Instantiate the model\n","  def __init__(self, learning_rate, tokenizer, model, freeze_encoder, freeze_embeds):\n","    super().__init__()\n","    self.tokenizer = tokenizer\n","    self.model = model\n","    self.learning_rate = learning_rate\n","    #self.hparams = hparams\n","    self.freeze_encoder = freeze_encoder\n","    self.freeze_embeds = freeze_embeds\n","\n","    if self.freeze_encoder:\n","      freeze_params(self.model.get_encoder())\n","\n","    if self.freeze_embeds:\n","      self.freeze_embeds()\n","  \n","  def freeze_embeds(self):\n","    ''' freeze the positional embedding parameters of the model; adapted from finetune.py '''\n","    freeze_params(self.model.model.shared)\n","    for d in [self.model.model.encoder, self.model.model.decoder]:\n","      freeze_params(d.embed_positions)\n","      freeze_params(d.embed_tokens)\n","\n","  # Do a forward pass through the model\n","  def forward(self, input_ids, **kwargs):\n","    return self.model(input_ids, **kwargs)\n","  \n","  def configure_optimizers(self):\n","    optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n","    return optimizer\n","\n","  def training_step(self, batch, batch_idx):\n","    # Load the data into variables\n","    src_ids, src_mask = batch[0], batch[1]\n","    tgt_ids = batch[2]\n","    # Shift the decoder tokens right (but NOT the tgt_ids)\n","    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n","\n","    # Run the model and get the logits\n","    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n","    lm_logits = outputs[0]\n","    # Create the loss function\n","    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n","    # Calculate the loss on the un-shifted tokens\n","    loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n","\n","    return {'loss':loss}\n","\n","  def validation_step(self, batch, batch_idx):\n","\n","    src_ids, src_mask = batch[0], batch[1]\n","    tgt_ids = batch[2]\n","\n","    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n","    \n","    # Run the model and get the logits\n","    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n","    lm_logits = outputs[0]\n","\n","    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n","    val_loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n","\n","    return {'loss': val_loss}\n","  \n","  # Method that generates text using the T5ForConditionalGeneration's generate() method\n","  def generate_text(self, text, eval_beams, early_stopping = True, max_len = 512):\n","    ''' Function to generate text '''\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    #device = \"cpu\"\n","    #device = \"cuda\"\n","    generated_ids = self.model.generate(\n","        text[\"input_ids\"].to(device),\n","        attention_mask=text[\"attention_mask\"].to(device),\n","        num_beams= eval_beams,\n","        length_penalty=2.0,\n","        max_length = max_len,\n","        min_length= 120,\n","        no_repeat_ngram_size=3,\n","        early_stopping = early_stopping,\n","        decoder_start_token_id= self.tokenizer.eos_token_id\n","    )\n","    return [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=False) for w in generated_ids]\n","\n","def freeze_params(model):\n","  ''' Function that takes a model as input (or part of a model) and freezes the layers for faster training\n","      adapted from finetune.py '''\n","  for layer in model.parameters():\n","    layer.requires_grade = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1650855055625,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"3cUIEJBIjjNJ"},"outputs":[],"source":["# Create a dataloading module as per the PyTorch Lightning Docs\n","class SummaryDataModule(pl.LightningDataModule):\n","  def __init__(self, tokenizer, data_file, batch_size, num_examples = 7000):\n","    super().__init__()\n","    self.tokenizer = tokenizer\n","    self.data_file = data_file\n","    self.batch_size = batch_size\n","    self.num_examples = num_examples\n","  \n","  # Loads and splits the data into training, validation and test sets with a 60/20/20 split\n","  # Updated: training and validation data 80/20 split\n","  def prepare_data(self):\n","    self.data = pd.read_csv(self.data_file).dropna()[:self.num_examples]\n","\n","    self.data['source'].astype(str)\n","    self.data['target'].astype(str)\n","    # train and validation only\n","    #self.train, self.validate, self.test = np.split(self.data.sample(frac=1), [int(.6*len(self.data)), int(.8*len(self.data))])\n","    self.train = self.data.sample(frac=0.8,random_state=200) \n","    self.validate = self.data.drop(self.train.index)\n","\n","  # encode the sentences using the tokenizer  \n","  def setup(self, stage):\n","    self.train = encode_sentences(self.tokenizer, self.train['source'], self.train['target'])\n","    self.validate = encode_sentences(self.tokenizer, self.validate['source'], self.validate['target'])\n","    #self.test = encode_sentences(self.tokenizer, self.test['source'], self.test['target'])\n","\n","  # Load the training, validation and test sets in Pytorch Dataset objects\n","  def train_dataloader(self):\n","    dataset = TensorDataset(self.train['input_ids'], self.train['attention_mask'], self.train['labels'])                          \n","    train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n","    return train_data\n","\n","  def val_dataloader(self):\n","    dataset = TensorDataset(self.validate['input_ids'], self.validate['attention_mask'], self.validate['labels']) \n","    val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n","    return val_data\n","\n","  # def test_dataloader(self):\n","  #   dataset = TensorDataset(self.test['input_ids'], self.test['attention_mask'], self.test['labels']) \n","  #   test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n","  #   return test_data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1650855055626,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"no6DwOqaE9Jw"},"outputs":[],"source":["def shift_tokens_right(input_ids, pad_token_id):\n","  \"\"\" Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\n","      This is taken directly from modeling_bart.py\n","  \"\"\"\n","  prev_output_tokens = input_ids.clone()\n","  index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n","  prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n","  prev_output_tokens[:, 1:] = input_ids[:, :-1]\n","  return prev_output_tokens\n","\n","def encode_sentences(tokenizer, source_sentences, target_sentences, max_length=512, pad_to_max_length=True, return_tensors=\"pt\"):\n","  ''' Function that tokenizes a sentence \n","      Args: tokenizer - the T5 tokenizer; source and target sentences are the source and target sentences\n","      Returns: Dictionary with keys: input_ids, attention_mask, target_ids\n","  '''\n","\n","  input_ids = []\n","  attention_masks = []\n","  target_ids = []\n","  tokenized_sentences = {}\n","\n","  for sentence in source_sentences:\n","    encoded_dict = tokenizer(\n","          sentence,\n","          max_length=max_length,\n","          padding=\"max_length\" if pad_to_max_length else None,\n","          truncation=True,\n","          return_tensors=return_tensors\n","      )\n","\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","  input_ids = torch.cat(input_ids, dim = 0)\n","  attention_masks = torch.cat(attention_masks, dim = 0)\n","\n","  for sentence in target_sentences:\n","    encoded_dict = tokenizer(\n","          str(sentence),\n","          max_length=max_length,\n","          padding=\"max_length\" if pad_to_max_length else None,\n","          truncation=True,\n","          return_tensors=return_tensors\n","      )\n","    # Shift the target ids to the right\n","    # shifted_target_ids = shift_tokens_right(encoded_dict['input_ids'], tokenizer.pad_token_id)\n","    target_ids.append(encoded_dict['input_ids'])\n","\n","  target_ids = torch.cat(target_ids, dim = 0)\n","  \n","\n","  batch = {\n","      \"input_ids\": input_ids,\n","      \"attention_mask\": attention_masks,\n","      \"labels\": target_ids,\n","  }\n","\n","  return batch\n","\n","\n","def noise_sentence(sentence_, percent_words, replacement_token = \"<mask>\"):\n","  '''\n","  Function that noises a sentence by adding <mask> tokens\n","  Args: sentence - the sentence to noise\n","        percent_words - the percent of words to replace with <mask> tokens; the number is rounded up using math.ceil\n","  Returns a noised sentence\n","  '''\n","  # Create a list item and copy\n","  sentence_ = sentence_.split(' ')\n","  sentence = sentence_.copy()\n","  \n","  num_words = math.ceil(len(sentence) * percent_words)\n","  \n","  # Create an array of tokens to sample from\n","  sample_tokens = set(np.arange(0, np.maximum(1, len(sentence)-1)))\n","  \n","  words_to_noise = random.sample(sample_tokens, num_words)\n","  \n","  # Swap out words, but not full stops\n","  for pos in words_to_noise:\n","      if sentence[pos] != '.':\n","          sentence[pos] = replacement_token\n","  \n","  # Remove redundant spaces\n","  sentence = re.sub(r' {2,5}', ' ', ' '.join(sentence))\n","  \n","  # Combine concurrent <mask> tokens into a single token; this just does two rounds of this; more could be done\n","  sentence = re.sub(r'<mask> <mask>', \"<mask>\", sentence)\n","  sentence = re.sub(r'<mask> <mask>', \"<mask>\", sentence)\n","  return sentence\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18524,"status":"ok","timestamp":1650855074135,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"sAKO4lHfiJMP","outputId":"55b666cf-aa74-46fa-86e2-e39bfe7a9b98"},"outputs":[],"source":["# Load the model\n","!pip install sentencepiece\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","\n","t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1650855074136,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"_h8QhLcyh9RJ"},"outputs":[],"source":["# Load the data into the model for training\n","data_path = root_dir + \"setup6_training.csv\"\n","summary_data = SummaryDataModule(tokenizer, data_path,\n","                                 batch_size = 3)\n","\n","# Load the model from a pre-saved checkpoint; alternatively use the code below to start training from scratch\n","# model = LitModel.load_from_checkpoint(base_dir + \"checkpoint_files_2/8_ep_140k_simple_0210.ckpt\",\n","#                                       learning_rate = 2e-5, tokenizer = tokenizer, model = bart_model, hparams = hparams)\n","\n","custom_model = LitModel(learning_rate = 2e-5, tokenizer = tokenizer, model = t5_model, freeze_encoder = True, freeze_embeds = False)"]},{"cell_type":"markdown","metadata":{"id":"2xcEqNMdGa6i"},"source":["# Training the model with Pytorch Lightning\n","The below code utilises Pytorch Lightning's fantastic Trainer module that helps to control the training process. After creating a ModelCheckpoint object, the other options are fed into the Trainer module. I found that my colab crashed when I didn't explicitly set progress_bar_refresh_rate to something and I found that setting it to 500 seemed to work just fine."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1650855074136,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"qAj9wgyRXbRG"},"outputs":[],"source":["is_training = False\n","if is_training:\n","  checkpoint = ModelCheckpoint(dirpath=base_dir + 'checkpoint_files/')\n","  trainer = pl.Trainer(gpus = 1,\n","                      max_epochs = 3,\n","                      min_epochs = 3,\n","                      auto_lr_find = False,\n","                      checkpoint_callback = checkpoint,\n","                      progress_bar_refresh_rate = 500)\n","  print(summary_data)\n","  # Fit the instantiated model to the data\n","  trainer.fit(custom_model, summary_data)\n","  torch.save(custom_model, base_dir + \"models/t5_pretrained_setup6_training_model_april_23_beam_10.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":476,"status":"ok","timestamp":1650855074596,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"Ffw_cKA0VC-Q"},"outputs":[],"source":["#import re\n","import pickle\n","from datetime import datetime\n","# import copy\n","import csv\n","\n","DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","#DEFAULT_DEVICE = \"cpu\"\n","#DEFAULT_DEVICE = \"cuda\"\n","def create_csv(all_sentences, targets, file_to_write):\n","  sources = []\n","  fieldnames = [\"source\", \"target\"]\n","  test_array = []\n","  with open(file_to_write, 'w') as csvfile:\n","    csvwriter = csv.DictWriter(csvfile, delimiter=',', fieldnames=fieldnames)\n","    for t in range(len(targets)):\n","        test_array.append({\"source\": all_sentences[t], \"target\": targets[t]})\n","        #writer.writerow({sources[t], targets[t]})\n","    csvwriter.writerow(dict((fn,fn) for fn in fieldnames))\n","    for row in test_array:\n","      csvwriter.writerow(row)\n","\n","\n","def chunks(lst, n):\n","    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n","    for i in range(0, len(lst), n):\n","        yield lst[i : i + n]\n","\n","def generate_summaries(lns, metric, batch_size=1, device=DEFAULT_DEVICE):\n","\n","    t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n","    #device = torch.device('cuda:0')\n","    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","    model = t5_model\n","    is_training = False\n","    if is_training:\n","      model = custom_model.to(device)\n","    \n","    article_batches = list(chunks(lns['source'], batch_size))\n","    target_batches = list(chunks(lns['target'], batch_size))\n","    ls_prediction = []\n","    ls_groundtruth = []\n","\n","    dec_batches_untokenized = []\n","    target_batches_untokenized = []\n","\n","    for article_batch, target_batch in tqdm(zip(article_batches, target_batches)\n","    , total=len(article_batches)):\n","        dct = tokenizer.batch_encode_plus(article_batch,\n","                                          max_length=1024,\n","                                          truncation=True,\n","                                          padding='max_length',\n","                                          return_tensors=\"pt\")\n","        dec = []\n","        if is_training:\n","          dec = model.generate_text(dct,10)\n","        else:\n","          summaries = model.generate(\n","              input_ids=dct[\"input_ids\"].to(device),\n","              attention_mask=dct[\"attention_mask\"].to(device),\n","              num_beams=10,\n","              length_penalty=2.0,\n","              max_length=512,\n","              min_length=120,\n","              no_repeat_ngram_size=3,\n","              early_stopping=True,\n","              decoder_start_token_id=tokenizer.eos_token_id,\n","          )\n","          dec = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]  \n","        dec = [d.replace('. ', '.\\n') for d in dec]\n","\n","        dec_batches_untokenized.append(dec)\n","        target_batches_untokenized.append(target_batch)\n","        \n","\n","        ls_prediction.extend(dec)\n","        ls_groundtruth.extend(target_batch)\n","\n","    \n","    \n","    ls_prediction_tokenized = coreNLP_tokenizer(ls_prediction)\n","    target_batch_tokenized = coreNLP_tokenizer(ls_groundtruth)\n","\n","    #for i in range(len(ls_prediction_tokenized)):\n","    # print(ls_prediction_tokenized[i])\n","    # print (target_batch_tokenized[i])\n","    # print('==============================')\n","    #print (ls_prediction_tokenized)\n","    #print(target_batch_tokenized)\n","    #dec_batches = list(chunks(ls_prediction_tokenized, batch_size))\n","    #target_batches = list(chunks(target_batch_tokenized, batch_size))\n","\n","\n","    #for dec_batch, target_batch in tqdm(zip(dec_batches, target_batches), total=len(dec_batches)):\n","    metric.add_batch(predictions=ls_prediction_tokenized, references=target_batch_tokenized)\n","\n","    score = metric.compute()\n","    str_now = str(datetime.now())\n","    result_file_name = root_dir + \"t5_pretrained_setup6_testing_custom_model_generated_summaries_april_24_beam_10.csv\"\n","    create_csv(ls_groundtruth, ls_prediction, result_file_name)\n","    #with open('/content/gdrive/Shareddrives/Informed Consent/202012_summarization_results/{0}_predictions.pkl'.format(str_now), 'wb') as fid:\n","    #    pickle.dump(ls_prediction, fid)\n","    #with open('/content/gdrive/Shareddrives/Informed Consent/202012_summarization_results/{0}_groundtruth.pkl'.format(str_now), 'wb') as fid:\n","    #    pickle.dump(ls_groundtruth, fid)\n","    print(\"ls prediction: \")\n","    print(ls_prediction)\n","    print(\"ls groundtruth: \")\n","    print(ls_groundtruth)\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7160,"status":"ok","timestamp":1650855081748,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"RrrLmv5TVVCw","outputId":"09cc44ed-1654-4d52-8f3b-093087d5c2e7"},"outputs":[],"source":["!pip install rouge_score\n","from datasets import list_metrics\n","metrics_list = list_metrics()\n","len(metrics_list)\n","print (metrics_list)\n","rouge_metric = load_metric('rouge')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3298,"status":"ok","timestamp":1650855085042,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"m7NPOqR0VXio","outputId":"a74e9c8f-6f90-4554-bfd3-785f2c9e23a2"},"outputs":[],"source":["# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n","!pip install stanza\n","\n","# Import stanza\n","import stanza"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1650855085043,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"MeXIeH4KVagi","outputId":"2c02b497-afe5-41d1-8cb5-ff40a3940cd0"},"outputs":[],"source":["# Download the Stanford CoreNLP package with Stanza's installation command\n","# This'll take several minutes, depending on the network speed\n","corenlp_dir = './corenlp'\n","stanza.install_corenlp(dir=corenlp_dir)\n","\n","# Set the CORENLP_HOME environment variable to point to the installation location\n","import os\n","os.environ[\"CORENLP_HOME\"] = corenlp_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1128,"status":"ok","timestamp":1650855086167,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"3BDzMmGeVcUG","outputId":"f1a44efa-470e-4ae4-e206-5549b315eddd"},"outputs":[],"source":["# Examine the CoreNLP installation folder to make sure the installation is successful\n","!export CORENLP_HOME='./corenlp'\n","!ls $CORENLP_HOME"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1650855086168,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"YljMK3xeVeE4"},"outputs":[],"source":["# Import client module\n","from stanza.server import CoreNLPClient"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32680,"status":"ok","timestamp":1650855118843,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"OTL8XULCVf9_","outputId":"688bc3c9-70d7-4196-dc47-f0187a12f602"},"outputs":[],"source":["texts = [\"Albert Einstein was a German-born theoretical physicist.\", \"He was going to the school!\"]\n","def coreNLP_tokenizer(inputDocsList):\n","  tokenizedDocsList = []\n","  with CoreNLPClient(annotators=\"tokenize ssplit pos lemma ner depparse\".split(), memory='4G', endpoint='http://localhost:9001', be_quiet=True) as client:\n","    for d in inputDocsList:\n","      ann = client.annotate(d)\n","\n","      # You can access annotations using ann.\n","      sentence = ann.sentence[0]\n","\n","      # You can access any property within a sentence.\n","      #print(sentence.text)\n","\n","      # Likewise for tokens\n","      #token = sentence.token[0]\n","      #print (token)\n","      tokenizedDocsList.append(' '.join([token.word.lower() for token in sentence.token]))\n","  return tokenizedDocsList\n","\n","print(coreNLP_tokenizer(texts))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6814275,"status":"ok","timestamp":1650861933105,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"hKLOTRiyVhnM","outputId":"91ea2816-cebc-41ff-efdd-31c7c860be75"},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv(root_dir + \"setup6_testing.csv\")\n","df.dropna()\n","df['source'] = df['source'].astype(str)\n","df['target'] = df['target'].astype(str)\n","df = df.iloc[:1000,:]\n","score = generate_summaries(df, rouge_metric) #tokenizer, bart_model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":721,"status":"ok","timestamp":1650861933106,"user":{"displayName":"Koyena Pal","userId":"01770458428262637467"},"user_tz":240},"id":"VoG5Ld_NVjdb","outputId":"d7ff286f-5bdb-48d1-aa7c-5f0a94658bfe"},"outputs":[],"source":["print(score)\n","#######.......//////////.....////"]},{"cell_type":"markdown","metadata":{"id":"48SUkaQas__L"},"source":["setup 4 in t5 (first 1000):\n","{'rouge1': AggregateScore(low=Score(precision=0.2588368848446092, recall=0.25122987056987783, fmeasure=0.24159818208918032), mid=Score(precision=0.26938102512722606, recall=0.26047505143685223, fmeasure=0.25007490042949343), high=Score(precision=0.28046001420673206, recall=0.2692709678688867, fmeasure=0.25917614865595223)), 'rouge2': AggregateScore(low=Score(precision=0.11255725678461885, recall=0.10476904845262759, fmeasure=0.10241127135610223), mid=Score(precision=0.12159803173368736, recall=0.11288291429396852, fmeasure=0.11012266509929843), high=Score(precision=0.13010995021653968, recall=0.12099060585571902, fmeasure=0.11759502895433997)), 'rougeL': AggregateScore(low=Score(precision=0.23253059700324577, recall=0.22681866514459234, fmeasure=0.21757340423495033), mid=Score(precision=0.24297955804783145, recall=0.23621253829663416, fmeasure=0.22585626245431717), high=Score(precision=0.2543204353100846, recall=0.24520426381322527, fmeasure=0.234950310186456)), 'rougeLsum': AggregateScore(low=Score(precision=0.23292684022002408, recall=0.22692990905414706, fmeasure=0.21789613310281677), mid=Score(precision=0.24349228577121668, recall=0.23603027794796122, fmeasure=0.22615060752782296), high=Score(precision=0.2536384238150224, recall=0.245574382028144, fmeasure=0.23461694249608914))}"]},{"cell_type":"markdown","metadata":{"id":"64ySYSGQmTfW"},"source":["changed to 6 epoch"]},{"cell_type":"markdown","metadata":{"id":"3ouuiCBbb74K"},"source":["setup 4 in t5 (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.24532993130828096, recall=0.23419495869906115, fmeasure=0.22629296089386128), mid=Score(precision=0.2549001763718447, recall=0.24293340467657282, fmeasure=0.23458867826333243), high=Score(precision=0.26570077183329144, recall=0.2516751389195236, fmeasure=0.24316837664654664)), 'rouge2': AggregateScore(low=Score(precision=0.10423858436663155, recall=0.09565089473538853, fmeasure=0.09407981394057427), mid=Score(precision=0.11191813321289143, recall=0.10258799830516005, fmeasure=0.1005735296715049), high=Score(precision=0.12023558417894827, recall=0.10970805845506043, fmeasure=0.10743926538791972)), 'rougeL': AggregateScore(low=Score(precision=0.22197892144095474, recall=0.21261308592646552, fmeasure=0.20484986814970946), mid=Score(precision=0.23160497177345007, recall=0.220859012872395, fmeasure=0.21287705676699625), high=Score(precision=0.24175020241453177, recall=0.22947441018493234, fmeasure=0.2209461440380066)), 'rougeLsum': AggregateScore(low=Score(precision=0.22158425247935534, recall=0.21200687228817555, fmeasure=0.20502817338804402), mid=Score(precision=0.23150246217664958, recall=0.2208251204600968, fmeasure=0.21287791895767444), high=Score(precision=0.2409962145802621, recall=0.22988400549480406, fmeasure=0.22126249686953672))}\n"]},{"cell_type":"markdown","metadata":{"id":"tbACCb9lFP0y"},"source":["setup 6 in t5 (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.2734287542926567, recall=0.2560322198466661, fmeasure=0.25134801112034877), mid=Score(precision=0.2836585496954529, recall=0.26475044966664607, fmeasure=0.2593615284826105), high=Score(precision=0.29395048853554884, recall=0.273451584557087, fmeasure=0.26737474721328086)), 'rouge2': AggregateScore(low=Score(precision=0.11937515196316832, recall=0.10741783714732911, fmeasure=0.10731777287275295), mid=Score(precision=0.12763559115106982, recall=0.11402597242108645, fmeasure=0.11393743257377573), high=Score(precision=0.13606010007446168, recall=0.12157016516965323, fmeasure=0.12102690058365138)), 'rougeL': AggregateScore(low=Score(precision=0.2468508258387302, recall=0.23163107438130756, fmeasure=0.22715665377144975), mid=Score(precision=0.25694137699027964, recall=0.2405508046403888, fmeasure=0.23491926467522134), high=Score(precision=0.2669517594507381, recall=0.2482987064499052, fmeasure=0.24306475182805207)), 'rougeLsum': AggregateScore(low=Score(precision=0.2471958001378389, recall=0.23238549149887847, fmeasure=0.22713919499685092), mid=Score(precision=0.25694767376473054, recall=0.24040916821297198, fmeasure=0.23498766678060393), high=Score(precision=0.26666459105940304, recall=0.24896723054549014, fmeasure=0.24277021987299077))}"]},{"cell_type":"markdown","metadata":{"id":"cTOl-DY4N4vy"},"source":["changed encode sentence max length to 512, num of training epochs to 3, and beam number to be 10"]},{"cell_type":"markdown","metadata":{"id":"B8XECaz8wgaf"},"source":["setup 1 t5 all (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.5255436538642831, recall=0.31372830779483274, fmeasure=0.3741903870515088), mid=Score(precision=0.5381175478122207, recall=0.322924044736426, fmeasure=0.38310615596935815), high=Score(precision=0.5505313341326222, recall=0.33165142893557314, fmeasure=0.39250453240400385)), 'rouge2': AggregateScore(low=Score(precision=0.32402092911145985, recall=0.19225491066135683, fmeasure=0.22999322301978498), mid=Score(precision=0.33668758016172284, recall=0.1996699962939506, fmeasure=0.23838104409281535), high=Score(precision=0.3502850407120836, recall=0.20818032169942782, fmeasure=0.2485942135992869)), 'rougeL': AggregateScore(low=Score(precision=0.47738684407827436, recall=0.2847449466731032, fmeasure=0.3398285211832176), mid=Score(precision=0.4907681752111453, recall=0.2937962918619609, fmeasure=0.3490004228674), high=Score(precision=0.5033991493536203, recall=0.3024189565071685, fmeasure=0.3583983921086392)), 'rougeLsum': AggregateScore(low=Score(precision=0.4788480166813028, recall=0.2847086027338496, fmeasure=0.3397681813523527), mid=Score(precision=0.4906212661613313, recall=0.29327093631127077, fmeasure=0.34880545339702745), high=Score(precision=0.5030227994090598, recall=0.30252853418064973, fmeasure=0.3582996187283443))}\n"]},{"cell_type":"markdown","metadata":{"id":"0Y2XmoLit9p3"},"source":["setup 7 t5 all (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.37224088131233374, recall=0.1580935928744946, fmeasure=0.21365619290578733), mid=Score(precision=0.3875441188474467, recall=0.16527337388357893, fmeasure=0.22294215895952463), high=Score(precision=0.4032160889651148, recall=0.1721600229296267, fmeasure=0.23221812561668298)), 'rouge2': AggregateScore(low=Score(precision=0.20553078959477716, recall=0.08491251307415748, fmeasure=0.11641891818497116), mid=Score(precision=0.2199322191856558, recall=0.09143309296161443, fmeasure=0.12507105537820828), high=Score(precision=0.23641600061115342, recall=0.09845427011167458, fmeasure=0.13485539730184978)), 'rougeL': AggregateScore(low=Score(precision=0.30563901257556336, recall=0.1289683578483327, fmeasure=0.17518982612025216), mid=Score(precision=0.32060140999465314, recall=0.1353417214372577, fmeasure=0.18352570811852528), high=Score(precision=0.33666192346098617, recall=0.14291469117687597, fmeasure=0.1934991766136125)), 'rougeLsum': AggregateScore(low=Score(precision=0.3060298257396111, recall=0.1289780963553509, fmeasure=0.17496327088771826), mid=Score(precision=0.321326729799685, recall=0.13562040613921328, fmeasure=0.18394195588391662), high=Score(precision=0.3367126767363716, recall=0.14219119987993198, fmeasure=0.192790284383627))}\n"]},{"cell_type":"markdown","metadata":{"id":"f1VI-8PkerKe"},"source":["setup 8 t5 all (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.6162713632909924, recall=0.1266557737201762, fmeasure=0.20605166018056056), mid=Score(precision=0.6339809118990576, recall=0.13095583154030843, fmeasure=0.21277389794421325), high=Score(precision=0.6493731388658893, recall=0.13494452572805915, fmeasure=0.21856212595048505)), 'rouge2': AggregateScore(low=Score(precision=0.5067363618634424, recall=0.09757335355860502, fmeasure=0.16067631099166677), mid=Score(precision=0.5313657575333209, recall=0.10266901101255266, fmeasure=0.1689853905681598), high=Score(precision=0.5543027252648117, recall=0.10787953061726172, fmeasure=0.17693813972150899)), 'rougeL': AggregateScore(low=Score(precision=0.5855503497000452, recall=0.12062974455020485, fmeasure=0.19619373194419093), mid=Score(precision=0.6041583161719428, recall=0.1251030815223586, fmeasure=0.20307408561649948), high=Score(precision=0.6231587890308702, recall=0.12993460821807917, fmeasure=0.21056610816933352)), 'rougeLsum': AggregateScore(low=Score(precision=0.5863443055978745, recall=0.12060796871360516, fmeasure=0.19664766743783946), mid=Score(precision=0.6047781067306466, recall=0.12536234315443476, fmeasure=0.20356775294913482), high=Score(precision=0.6229572549602016, recall=0.12967396359670952, fmeasure=0.21026593302354818))}\n"]},{"cell_type":"markdown","metadata":{"id":"ChTE0RuOc4_-"},"source":["setup 4 t5 all (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.3582225240192015, recall=0.347989443175754, fmeasure=0.33544990336941205), mid=Score(precision=0.3752857390724563, recall=0.3655119596419827, fmeasure=0.35231212769568865), high=Score(precision=0.3918719314706983, recall=0.3822758862147936, fmeasure=0.36887532587875)), 'rouge2': AggregateScore(low=Score(precision=0.21260369953206304, recall=0.2070055755763381, fmeasure=0.20090761964949902), mid=Score(precision=0.23073571658980224, recall=0.22430266903179868, fmeasure=0.21831471299258756), high=Score(precision=0.2482252005195825, recall=0.24252897591685055, fmeasure=0.2361606852944207)), 'rougeL': AggregateScore(low=Score(precision=0.3323185971142232, recall=0.3240178716149586, fmeasure=0.3126665256272832), mid=Score(precision=0.3491036220993431, recall=0.34072992220070963, fmeasure=0.3282461689626657), high=Score(precision=0.3676753645270214, recall=0.35750524964648295, fmeasure=0.3451881159306764)), 'rougeLsum': AggregateScore(low=Score(precision=0.3309744701221845, recall=0.32283049947498005, fmeasure=0.3118686679753952), mid=Score(precision=0.3489514882415937, recall=0.34067494121398906, fmeasure=0.3282679893382323), high=Score(precision=0.3674213554274379, recall=0.3563216250707289, fmeasure=0.34436605566047596))}\n"]},{"cell_type":"markdown","metadata":{"id":"9d6b181MEZMJ"},"source":["setup 6 t5 all (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.39011294210532266, recall=0.3905932170145604, fmeasure=0.3721957905579733), mid=Score(precision=0.4077396391683317, recall=0.4066657422841817, fmeasure=0.3880129714768519), high=Score(precision=0.4269929298713382, recall=0.4246648390411941, fmeasure=0.4057429048499838)), 'rouge2': AggregateScore(low=Score(precision=0.24701839990747487, recall=0.24193062860804307, fmeasure=0.23479748287996438), mid=Score(precision=0.2667013425820912, recall=0.26051425091990693, fmeasure=0.25376123451317867), high=Score(precision=0.28577782466587975, recall=0.2781975122448364, fmeasure=0.27138393665583205)), 'rougeL': AggregateScore(low=Score(precision=0.3640200665043463, recall=0.3634759813666907, fmeasure=0.34668810897366464), mid=Score(precision=0.38248024610129805, recall=0.38056506032879867, fmeasure=0.36422830971145115), high=Score(precision=0.4022069836191019, recall=0.39933484425572113, fmeasure=0.3825026916079375)), 'rougeLsum': AggregateScore(low=Score(precision=0.3650504829316673, recall=0.3645361015485961, fmeasure=0.34847448241503554), mid=Score(precision=0.3827158199117455, recall=0.3817142913721612, fmeasure=0.3651245573212353), high=Score(precision=0.401213920761873, recall=0.3981881946674005, fmeasure=0.38160681457297796))}\n"]},{"cell_type":"markdown","metadata":{"id":"hvtCHTbhd_sp"},"source":["setup 1 pretrain (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.03259547786872938, recall=0.0016706522989682635, fmeasure=0.0029989873955616644), mid=Score(precision=0.041912725829529436, recall=0.002135173317364351, fmeasure=0.003816674941054876), high=Score(precision=0.05108167583584909, recall=0.0025878158759678254, fmeasure=0.004553363355053297)), 'rouge2': AggregateScore(low=Score(precision=7.125576036866374e-05, recall=1.3333333333333335e-05, fmeasure=2.5641025641025643e-05), mid=Score(precision=0.0018043394777265743, recall=6.285159500693481e-05, fmeasure=0.00010952260458839406), high=Score(precision=0.004774020737327189, recall=0.0001223517643163274, fmeasure=0.00021905357328692952)), 'rougeL': AggregateScore(low=Score(precision=0.029875902676496285, recall=0.0015133829156594987, fmeasure=0.0027204596805181685), mid=Score(precision=0.03935367047630661, recall=0.0019008395455825744, fmeasure=0.0033935768353228063), high=Score(precision=0.04898549118569132, recall=0.0022851825280042733, fmeasure=0.004084403433700073)), 'rougeLsum': AggregateScore(low=Score(precision=0.030114075896663015, recall=0.0014895098226557846, fmeasure=0.0026845839274942575), mid=Score(precision=0.03884936370146141, recall=0.0018788242205798758, fmeasure=0.0033577439734692825), high=Score(precision=0.048762832769284295, recall=0.0022918990703361818, fmeasure=0.00406835296167083))}\n"]},{"cell_type":"markdown","metadata":{"id":"2qTEaN7HYuqC"},"source":["setup 7 pretrain (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.030992870246383673, recall=0.0024665710321339, fmeasure=0.003998513489365846), mid=Score(precision=0.03964802378455144, recall=0.003187447607871147, fmeasure=0.005019866082347868), high=Score(precision=0.0494274982070367, recall=0.003921897800185747, fmeasure=0.006003897165363594)), 'rouge2': AggregateScore(low=Score(precision=0.0004905393217893218, recall=8.262796799910006e-05, fmeasure=0.00013138530684600477), mid=Score(precision=0.0019618559286242216, recall=0.00016287886758631711, fmeasure=0.00026777642248022865), high=Score(precision=0.004625879183540473, recall=0.0002768266685804522, fmeasure=0.00045236160324821926)), 'rougeL': AggregateScore(low=Score(precision=0.02927608742165716, recall=0.002236362588623754, fmeasure=0.0036176224555653893), mid=Score(precision=0.03746676149329313, recall=0.002808029673002293, fmeasure=0.004433425646349449), high=Score(precision=0.047090823039561275, recall=0.003454211466429874, fmeasure=0.005346585818330104)), 'rougeLsum': AggregateScore(low=Score(precision=0.02884691663976738, recall=0.0022007259006459785, fmeasure=0.0035965532696290944), mid=Score(precision=0.03754718029131955, recall=0.002811135867842785, fmeasure=0.004421114709213617), high=Score(precision=0.047460064485485856, recall=0.0034396208807995297, fmeasure=0.005339585102256928))}\n"]},{"cell_type":"markdown","metadata":{"id":"XW7P8lNOOiHF"},"source":["setup 8 pretrain (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.03091683423860939, recall=0.0025142563393685713, fmeasure=0.004048965845486613), mid=Score(precision=0.03966581631230347, recall=0.0031814087644110875, fmeasure=0.005004501168629086), high=Score(precision=0.050309799221484014, recall=0.0039037898451492452, fmeasure=0.006030902945748073)), 'rouge2': AggregateScore(low=Score(precision=0.0005193102948389141, recall=7.563314896762498e-05, fmeasure=0.00012449489896024392), mid=Score(precision=0.0019243646422639343, recall=0.00016320793035253138, fmeasure=0.00026761681735320306), high=Score(precision=0.004461120566261695, recall=0.00026909629622543944, fmeasure=0.00044424131284745265)), 'rougeL': AggregateScore(low=Score(precision=0.02901157063436032, recall=0.0022090000322053586, fmeasure=0.00358546134796503), mid=Score(precision=0.03751561917476741, recall=0.0028053642592990435, fmeasure=0.004421083963753408), high=Score(precision=0.04713143385218611, recall=0.003447489984385766, fmeasure=0.005360962581723463)), 'rougeLsum': AggregateScore(low=Score(precision=0.028854213973295895, recall=0.002274766545794876, fmeasure=0.0036587949855803096), mid=Score(precision=0.03773258913978569, recall=0.002798515994299667, fmeasure=0.0044088888627801985), high=Score(precision=0.047390460587802274, recall=0.003442136768710669, fmeasure=0.0053193395999020995))}\n"]},{"cell_type":"markdown","metadata":{"id":"MM7wEO8Fn6Ro"},"source":["setup 4 pretrain (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.018925643670780676, recall=0.0032940894023704803, fmeasure=0.004759878955667924), mid=Score(precision=0.025762516790534776, recall=0.004282842559666898, fmeasure=0.006182844135155321), high=Score(precision=0.033473717841486464, recall=0.005462770556811458, fmeasure=0.007784247980584912)), 'rouge2': AggregateScore(low=Score(precision=1.1904761904761903e-05, recall=2.631578947368421e-05, fmeasure=1.7241379310344828e-05), mid=Score(precision=0.0004904761904761905, recall=0.0001388888888888889, fmeasure=0.00017733990147783253), high=Score(precision=0.001183630952380952, recall=0.00030266812865497076, fmeasure=0.00039819376026272574)), 'rougeL': AggregateScore(low=Score(precision=0.017579443225376197, recall=0.002958348445087641, fmeasure=0.0043371326903011865), mid=Score(precision=0.024789165278209603, recall=0.003936384145540256, fmeasure=0.00569274741028688), high=Score(precision=0.033054215119657636, recall=0.004953625313713798, fmeasure=0.007072376120417085)), 'rougeLsum': AggregateScore(low=Score(precision=0.017424819145004142, recall=0.002958173664274011, fmeasure=0.0042704284069091), mid=Score(precision=0.024909211160130975, recall=0.00390500235449091, fmeasure=0.0056597149714497155), high=Score(precision=0.03201574174040795, recall=0.004971711249632905, fmeasure=0.0070926954973514235))}\n"]},{"cell_type":"markdown","metadata":{"id":"9uXqapVyCjRX"},"source":["setup 6 pretrain (first 1000): {'rouge1': AggregateScore(low=Score(precision=0.02718192730880231, recall=0.0026717142148058636, fmeasure=0.0045562897201596566), mid=Score(precision=0.03591679015429016, recall=0.003489515114214574, fmeasure=0.00590481992109375), high=Score(precision=0.04599801587301587, recall=0.004392681221245657, fmeasure=0.007342675102130404)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0013, recall=0.0001253387533875339, fmeasure=0.0002265446224256293), high=Score(precision=0.0035666666666666663, recall=0.0003865282012195121, fmeasure=0.0006800343249427916)), 'rougeL': AggregateScore(low=Score(precision=0.024741345806970808, recall=0.0024378641268484927, fmeasure=0.004158288779800289), mid=Score(precision=0.03449874847374848, recall=0.003216194103782987, fmeasure=0.0054687982004725175), high=Score(precision=0.04444560592185592, recall=0.004065869224296131, fmeasure=0.006880409615877091)), 'rougeLsum': AggregateScore(low=Score(precision=0.025836417402042396, recall=0.0025075401660765747, fmeasure=0.004305123654097008), mid=Score(precision=0.03461459651459651, recall=0.003222597709792241, fmeasure=0.005451766170243225), high=Score(precision=0.04507080502830505, recall=0.004067099565297772, fmeasure=0.006880476218548522))}\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"T5 all.ipynb","provenance":[{"file_id":"16IuWua4UzpsxwcQ8v3NzVJS7aBTPC4qo","timestamp":1645406804867},{"file_id":"1WB-vzkNMZE4WQTixIAwBQrYIBsWPEkS8","timestamp":1644155761443},{"file_id":"1Cy27V-7qqYatqMA7fEqG2kgMySZXw9I4","timestamp":1635118510828},{"file_id":"14YV_dqwbasADzDpv2D3mxwc5fp2bnTXq","timestamp":1602946950398},{"file_id":"11nhGtBRdfhk070pkfQCG9vqr7-fjELfU","timestamp":1601665293162},{"file_id":"18a_Hsz2llHauu2zbUVvxOXncFWcm8qW5","timestamp":1601125027298},{"file_id":"1PE67Fsh3Iz5Bm4gK9YrXcMJWu5ukrYb5","timestamp":1600539357919}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
